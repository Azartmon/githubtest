{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPar0EdfYqJ0PPwXNOprknC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Azartmon/githubtest/blob/main/llamatest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index\n",
        "!pip install llama_index.embeddings.huggingface\n",
        "!pip install llama_index.llms.ollama\n",
        "!pip install llama_index.llms.fireworks"
      ],
      "metadata": {
        "id": "VBtarh8qblB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQmJxtdkZhOa",
        "outputId": "18e25eac-2c09-4b3d-9fb7-32a690a22598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what do you want to query?what is a ai?\n",
            "LLM with local index response:\n",
            "\n",
            " There is no information about what an AI is.\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "\n",
        "To import llama_index.llms.ollama, you should run pip install llama-index-llms-ollama.\n",
        "\n",
        "\n",
        "\n",
        "To import llama_index.embeddings.huggingface, you should run pip install llama-index-embeddings-huggingface.\n",
        "\n",
        "'''\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "from llama_index.llms.ollama import Ollama\n",
        "\n",
        "# from langchain_groq import ChatGroq\n",
        "\n",
        "from llama_index.llms.fireworks import Fireworks\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "os.environ['FIREWORKS_API_KEY'] = '8nozAPErfL1PgACJgLIsUzKFPgGqabQGAPJ4z0VogUeiZhSb' # get a free key at https://fireworks.ai/api-keys\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"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\"\n",
        "\n",
        "\n",
        "\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "\n",
        "\n",
        "# bge-base embedding model\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "\n",
        "\n",
        "# ollama\n",
        "\n",
        "local_llm = Ollama(model=\"llama3.1:8b\", request_timeout=360.0) #local run model via ollama\n",
        "\n",
        "#llm = Fireworks(model=\"accounts/fireworks/models/llama-v3-8b-instruct\", temperature=0)\n",
        "\n",
        "# llm = Fireworks(model=\"accounts/fireworks/models/llama-v3-70b-instruct\", temperature=0)\n",
        "\n",
        "firework_llm = Fireworks(model=\"accounts/fireworks/models/llama-v3p1-405b-instruct\", temperature=0)\n",
        "\n",
        "# groq_llm = ChatGroq(\n",
        "\n",
        "#     model=\"llama-3.1-70b-versatile\", #gemma2-9b-it\n",
        "\n",
        "#     # model=\"whisper-large-v3\", #gemma2-9b-it\n",
        "\n",
        "#     # model=\"llama-3.1-8b-instant\", #gemma2-9b-it\n",
        "\n",
        "#     temperature=0.7,\n",
        "\n",
        "#     max_tokens=1024,\n",
        "\n",
        "#     stop=None,\n",
        "\n",
        "# )\n",
        "\n",
        "Settings.llm = firework_llm\n",
        "\n",
        "\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "\n",
        "    documents,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "prompt = input(\"what do you want to query?\")\n",
        "\n",
        "response = query_engine.query(prompt)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"LLM with local index response:\\n\\n {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tic Tac Toe game in Python\n",
        "\n",
        "board = [' ' for _ in range(9)]\n",
        "\n",
        "def print_board():\n",
        "    row1 = '| {} | {} | {} |'.format(board[0], board[1], board[2])\n",
        "    row2 = '| {} | {} | {} |'.format(board[3], board[4], board[5])\n",
        "    row3 = '| {} | {} | {} |'.format(board[6], board[7], board[8])\n",
        "\n",
        "    print()\n",
        "    print(row1)\n",
        "    print(row2)\n",
        "    print(row3)\n",
        "    print()\n",
        "\n",
        "def check_win():\n",
        "    win_conditions = [(0, 1, 2), (3, 4, 5), (6, 7, 8), (0, 3, 6), (1, 4, 7), (2, 5, 8), (0, 4, 8), (2, 4, 6)]\n",
        "    for condition in win_conditions:\n",
        "        if board[condition[0]] == board[condition[1]] == board[condition[2]] != ' ':\n",
        "            return board[condition[0]]"
      ],
      "metadata": {
        "id": "meYkyeZ5bh6K"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}